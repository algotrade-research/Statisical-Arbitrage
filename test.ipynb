{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get VN Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from vnstock import Vnstock\n",
    "from typing import List, Dict, Tuple\n",
    "import numpy as np\n",
    "from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import os\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.api import AutoReg\n",
    "from typing import Dict\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_data(symbols, start_date, end_date, interval='1D'):\n",
    "    stock_data = pd.DataFrame()  # Initialize empty DataFrame\n",
    "    for symbol in symbols:\n",
    "        # Fetch historical data for the symbol\n",
    "        stock = Vnstock().stock(symbol=symbol, source='VCI')\n",
    "        historical_data = stock.quote.history(\n",
    "            start=start_date, \n",
    "            end=end_date, \n",
    "            interval=interval\n",
    "        )\n",
    "        # Set 'time' as the index and keep only the 'Close' column\n",
    "        close_prices = historical_data[['close', 'time']].set_index('time')\n",
    "        close_prices = close_prices.rename(columns={'close': symbol})  # Rename column to symbol\n",
    "        # Concatenate with the main DataFrame\n",
    "        stock_data = pd.concat([stock_data, close_prices], axis=1).dropna()\n",
    "    return stock_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatArbStrategy:\n",
    "    def __init__(self, etf, stocks, start_date, end_date, file_path,\n",
    "                 estimation_window=60, min_trading_days=30, threshold=0.05,\n",
    "                 max_stocks=6, confidence_level=1, confidence_level_joh_final=2,\n",
    "                 adf_significance=0.005, adf_significance_trading=0.05,\n",
    "                 correlation_threshold=0.8):\n",
    "        \"\"\"Initialize the statistical arbitrage strategy with parameters.\"\"\"\n",
    "        self.etf = etf\n",
    "        self.stocks = stocks\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date\n",
    "        self.file_path = file_path\n",
    "        self.estimation_window = estimation_window\n",
    "        self.min_trading_days = min_trading_days\n",
    "        self.threshold = threshold\n",
    "        self.max_stocks = max_stocks\n",
    "        self.confidence_level = confidence_level\n",
    "        self.confidence_level_joh_final = confidence_level_joh_final\n",
    "        self.adf_significance = adf_significance\n",
    "        self.adf_significance_trading = adf_significance_trading\n",
    "        self.correlation_threshold = correlation_threshold  # New parameter for similarity filtering\n",
    "        self.data = self.load_data()\n",
    "        self.active_combinations = []\n",
    "        self.combination_id = 0\n",
    "        self.results = []\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"Load stock data from file or fetch if not available.\"\"\"\n",
    "        if os.path.exists(self.file_path):\n",
    "            data = pd.read_csv(self.file_path)\n",
    "        else:\n",
    "            data = get_stock_data([self.etf] + self.stocks, self.start_date, self.end_date, '1D')\n",
    "            data.to_csv(self.file_path, index=True)\n",
    "        data['Date'] = pd.to_datetime(data['time'])\n",
    "        return data.set_index('Date')\n",
    "\n",
    "    def get_pairwise_candidates(self, window_data):\n",
    "        \"\"\"Perform pairwise Johansen tests to identify cointegrated stocks.\"\"\"\n",
    "        candidates = []\n",
    "        for stock in self.stocks:\n",
    "            try:\n",
    "                result = coint_johansen(window_data[[self.etf, stock]], det_order=0, k_ar_diff=1)\n",
    "                if result.lr1[0] > result.cvt[0, self.confidence_level]:\n",
    "                    candidates.append((stock, result.lr1[0]))\n",
    "            except Exception as e:\n",
    "                print(f\"Pairwise test failed for {stock}: {e}\")\n",
    "        candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "        return [stock for stock, _ in candidates]\n",
    "\n",
    "    def build_combination(self, window_data, filtered_stocks):\n",
    "        \"\"\"Incrementally build a cointegrated combination of stocks.\"\"\"\n",
    "        if not filtered_stocks:\n",
    "            return []\n",
    "        selected = [filtered_stocks[0]]\n",
    "        best_trace_stat = coint_johansen(window_data[[self.etf, selected[0]]], det_order=0, k_ar_diff=1).lr1[0]\n",
    "\n",
    "        for stock in filtered_stocks[1:]:\n",
    "            if len(selected) >= self.max_stocks:\n",
    "                break\n",
    "            if stock in selected:\n",
    "                continue\n",
    "            test_subset = selected + [stock]\n",
    "            try:\n",
    "                result = coint_johansen(window_data[[self.etf] + test_subset], det_order=0, k_ar_diff=0)\n",
    "                if result.lr1[0] <= result.cvt[0, self.confidence_level]:\n",
    "                    continue\n",
    "                evec = result.evec[:, 0]\n",
    "                betas = -evec[1:] / evec[0]\n",
    "                if not all(beta >= 0 for beta in betas) or result.lr1[0] <= best_trace_stat:\n",
    "                    continue\n",
    "                selected.append(stock)\n",
    "                best_trace_stat = result.lr1[0]\n",
    "            except Exception as e:\n",
    "                print(f\"Combination test failed: {e}\")\n",
    "        return selected\n",
    "\n",
    "    def validate_combination(self, window_data, selected):\n",
    "        \"\"\"Validate the combination with stricter criteria and ADF test.\"\"\"\n",
    "        if not selected:\n",
    "            return None, np.inf  # Return high p-value if no combination\n",
    "        try:\n",
    "            result = coint_johansen(window_data[[self.etf] + selected], det_order=0, k_ar_diff=1)\n",
    "            if result.lr1[0] <= result.cvt[0, self.confidence_level_joh_final]:\n",
    "                return None, np.inf\n",
    "            evec = result.evec[:, 0]\n",
    "            betas = -evec[1:] / evec[0]\n",
    "            if not all(beta >= 0 for beta in betas):\n",
    "                return None, np.inf\n",
    "            selected_betas = {s: b for s, b in zip(selected, betas) if abs(b) > self.threshold}\n",
    "            residuals = window_data[self.etf] - sum(window_data[s] * b for s, b in selected_betas.items())\n",
    "            adf_pvalue = adfuller(residuals)[1]\n",
    "            if adf_pvalue >= self.adf_significance:\n",
    "                return None, adf_pvalue\n",
    "            return selected_betas, adf_pvalue\n",
    "        except Exception as e:\n",
    "            print(f\"Final validation failed: {e}\")\n",
    "            return None, np.inf\n",
    "\n",
    "    def is_similar(self, new_residuals, existing_residuals):\n",
    "        \"\"\"Check if two residual series are highly correlated.\"\"\"\n",
    "        if len(new_residuals) != len(existing_residuals):\n",
    "            return False  # Cannot compare if lengths differ\n",
    "        corr, _ = pearsonr(new_residuals, existing_residuals)\n",
    "        return corr > self.correlation_threshold\n",
    "\n",
    "    def run_strategy(self):\n",
    "        \"\"\"Execute the rolling statistical arbitrage strategy with similarity filtering.\"\"\"\n",
    "        for day in range(self.estimation_window, len(self.data)):\n",
    "            estimation_data = self.data.iloc[day - self.estimation_window:day]\n",
    "            current_day = self.data.index[day]\n",
    "\n",
    "            # Generate potential new combination\n",
    "            filtered_stocks = self.get_pairwise_candidates(estimation_data)\n",
    "            selected = self.build_combination(estimation_data, filtered_stocks)\n",
    "            selected_betas, new_adf_pvalue = self.validate_combination(estimation_data, selected)\n",
    "\n",
    "            if selected_betas:\n",
    "                new_residuals = estimation_data[self.etf] - sum(\n",
    "                    estimation_data[s] * b for s, b in selected_betas.items()\n",
    "                )\n",
    "\n",
    "                # Check similarity with active combinations\n",
    "                similar_found = False\n",
    "                for comb in self.active_combinations:\n",
    "                    if self.is_similar(new_residuals, pd.Series(comb['all_residuals'][-self.estimation_window:])):\n",
    "                        # If similar, compare ADF p-values\n",
    "                        existing_adf_pvalue = adfuller(pd.Series(comb['all_residuals'][-self.estimation_window:]))[1]\n",
    "                        if new_adf_pvalue < existing_adf_pvalue:\n",
    "                            # Replace the existing combination with the new one\n",
    "                            self.active_combinations.remove(comb)\n",
    "                            break\n",
    "                        else:\n",
    "                            similar_found = True\n",
    "                            break\n",
    "\n",
    "                if not similar_found:\n",
    "                    self.combination_id += 1\n",
    "                    self.active_combinations.append({\n",
    "                        'id': self.combination_id,\n",
    "                        'betas': selected_betas,\n",
    "                        'start_day': day,\n",
    "                        'all_residuals': new_residuals.tolist(),\n",
    "                        'trading_days': 0\n",
    "                    })\n",
    "                    for i, res in enumerate(new_residuals):\n",
    "                        row = {\n",
    "                            'Date': estimation_data.index[i],\n",
    "                            'Combination_ID': self.combination_id,\n",
    "                            'Residual': res,\n",
    "                            'Total_Combinations': len(self.active_combinations),\n",
    "                            'Num_Stocks': len(selected_betas),\n",
    "                            'Is_Estimation': True,\n",
    "                            **{f'Beta_{s}': b for s, b in selected_betas.items()}\n",
    "                        }\n",
    "                        self.results.append(row)\n",
    "                    print(f\"\\n=== New Combination {self.combination_id} at {current_day.date()} ===\")\n",
    "                    print(\"VN30F1M = \" + \" + \".join([f\"{b:.3f}*{s}\" for s, b in selected_betas.items()]))\n",
    "\n",
    "            # Evaluate active combinations\n",
    "            for comb in self.active_combinations[:]:\n",
    "                if day < comb['start_day']:\n",
    "                    continue\n",
    "                comb['trading_days'] += 1\n",
    "                current_prices = self.data.iloc[day]\n",
    "                residual = current_prices[self.etf] - sum(\n",
    "                    current_prices[s] * b for s, b in comb['betas'].items()\n",
    "                )\n",
    "                comb['all_residuals'].append(residual)\n",
    "\n",
    "                if comb['trading_days'] >= self.min_trading_days:\n",
    "                    all_residuals_series = pd.Series(comb['all_residuals'])\n",
    "                    if adfuller(all_residuals_series, autolag='AIC')[1] >= self.adf_significance_trading:\n",
    "                        self.active_combinations.remove(comb)\n",
    "                        continue\n",
    "\n",
    "                row = {\n",
    "                    'Date': current_day,\n",
    "                    'Combination_ID': comb['id'],\n",
    "                    'Residual': residual,\n",
    "                    'Total_Combinations': len(self.active_combinations),\n",
    "                    'Num_Stocks': len(comb['betas']),\n",
    "                    'Is_Estimation': False,\n",
    "                    **{f'Beta_{s}': b for s, b in comb['betas'].items()}\n",
    "                }\n",
    "                self.results.append(row)\n",
    "\n",
    "    def get_results(self):\n",
    "        \"\"\"Return the results as a DataFrame.\"\"\"\n",
    "        self.results = pd.DataFrame(self.results)\n",
    "        self.results = self.results.sort_values(by=['Combination_ID','Date'])\n",
    "        return self.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== New Combination 1 at 2024-04-02 ===\n",
      "VN30F1M = 50.942*E1VFVN30 + 3.319*MBB\n",
      "\n",
      "=== New Combination 2 at 2024-04-23 ===\n",
      "VN30F1M = 47.353*E1VFVN30 + 10.305*FUESSVFL\n",
      "\n",
      "=== New Combination 3 at 2024-04-24 ===\n",
      "VN30F1M = 42.153*E1VFVN30 + 13.461*FUESSVFL + 0.561*GVR\n",
      "\n",
      "=== New Combination 4 at 2024-05-02 ===\n",
      "VN30F1M = 38.228*E1VFVN30 + 12.636*FUESSVFL + 0.620*VIC + 1.374*VCB + 0.897*HDB\n",
      "\n",
      "=== New Combination 5 at 2024-05-06 ===\n",
      "VN30F1M = 32.356*E1VFVN30 + 15.745*FUEVN100 + 0.611*VIC + 8.328*ACB + 0.348*VJC + 1.046*VCB\n",
      "\n",
      "=== New Combination 6 at 2024-05-07 ===\n",
      "VN30F1M = 33.143*E1VFVN30 + 16.065*FUEVN100 + 0.277*VIC + 6.302*ACB + 0.910*VCB + 2.999*HDB\n",
      "\n",
      "=== New Combination 7 at 2024-05-10 ===\n",
      "VN30F1M = 37.655*E1VFVN30 + 6.688*ACB + 14.571*FUEVN100 + 0.655*VCB + 2.484*HDB\n",
      "\n",
      "=== New Combination 8 at 2024-05-17 ===\n",
      "VN30F1M = 49.461*E1VFVN30 + 0.534*VIC + 12.338*FUEVN100\n",
      "\n",
      "=== New Combination 9 at 2024-05-20 ===\n",
      "VN30F1M = 60.181*E1VFVN30 + 0.561*VIC\n",
      "\n",
      "=== New Combination 10 at 2024-05-22 ===\n",
      "VN30F1M = 50.120*E1VFVN30 + 13.502*FUEVN100\n",
      "\n",
      "=== New Combination 11 at 2024-05-23 ===\n",
      "VN30F1M = 47.290*E1VFVN30 + 14.664*FUEVN100 + 2.310*ACB\n",
      "\n",
      "=== New Combination 12 at 2024-05-27 ===\n",
      "VN30F1M = 50.824*E1VFVN30 + 10.277*FUEVN100\n",
      "\n",
      "=== New Combination 13 at 2024-05-28 ===\n",
      "VN30F1M = 54.290*E1VFVN30 + 5.625*VPB\n",
      "\n",
      "=== New Combination 14 at 2024-06-25 ===\n",
      "VN30F1M = 6.362*MBB + 49.107*E1VFVN30\n",
      "\n",
      "=== New Combination 15 at 2024-07-03 ===\n",
      "VN30F1M = 9.207*MBB + 47.558*E1VFVN30 + 4.417*VNM\n",
      "\n",
      "=== New Combination 16 at 2024-07-04 ===\n",
      "VN30F1M = 0.790*MBB + 47.924*E1VFVN30\n",
      "\n",
      "=== New Combination 17 at 2024-07-12 ===\n",
      "VN30F1M = 15.563*MBB + 42.877*E1VFVN30 + 5.648*CTG\n",
      "\n",
      "=== New Combination 18 at 2024-07-16 ===\n",
      "VN30F1M = 40.701*MBB\n",
      "\n",
      "=== New Combination 19 at 2024-07-22 ===\n",
      "VN30F1M = 46.975*FUESSVFL\n",
      "\n",
      "=== New Combination 20 at 2024-08-08 ===\n",
      "VN30F1M = 58.627*E1VFVN30 + 1.440*VIC + 3.542*VPB\n",
      "\n",
      "=== New Combination 21 at 2024-08-09 ===\n",
      "VN30F1M = 65.940*E1VFVN30 + 1.019*VIC\n",
      "\n",
      "=== New Combination 22 at 2024-08-22 ===\n",
      "VN30F1M = 56.394*E1VFVN30 + 1.501*VHM + 5.990*TPB\n",
      "\n",
      "=== New Combination 23 at 2024-09-05 ===\n",
      "VN30F1M = 49.761*E1VFVN30 + 0.738*TCB + 10.009*TPB + 0.545*FPT\n",
      "\n",
      "=== New Combination 24 at 2024-09-06 ===\n",
      "VN30F1M = 56.706*E1VFVN30 + 1.072*TCB + 6.292*TPB\n",
      "\n",
      "=== New Combination 25 at 2024-10-07 ===\n",
      "VN30F1M = 14.004*MBB + 0.465*FPT + 3.301*BCM + 42.678*E1VFVN30\n",
      "\n",
      "=== New Combination 26 at 2024-10-08 ===\n",
      "VN30F1M = 13.297*GVR + 19.384*MBB + 2.614*BCM + 5.739*TCB\n",
      "\n",
      "=== New Combination 27 at 2024-10-10 ===\n",
      "VN30F1M = 7.309*FPT + 11.079*TCB\n",
      "\n",
      "=== New Combination 28 at 2024-10-15 ===\n",
      "VN30F1M = 29.289*MBB + 11.064*GVR + 0.308*BCM + 2.313*TCB\n",
      "\n",
      "=== New Combination 29 at 2024-10-21 ===\n",
      "VN30F1M = 41.401*E1VFVN30 + 6.497*SSI + 1.753*GVR\n",
      "\n",
      "=== New Combination 30 at 2024-11-13 ===\n",
      "VN30F1M = 92.376*CTG\n",
      "\n",
      "=== New Combination 31 at 2024-11-15 ===\n",
      "VN30F1M = 37.706*MBB + 2.008*SSI\n",
      "\n",
      "=== New Combination 32 at 2024-11-21 ===\n",
      "VN30F1M = 60.468*E1VFVN30\n",
      "\n",
      "=== New Combination 33 at 2024-11-22 ===\n",
      "VN30F1M = 61.171*E1VFVN30\n",
      "\n",
      "=== New Combination 34 at 2024-12-04 ===\n",
      "VN30F1M = 18.295*GAS\n",
      "\n",
      "=== New Combination 35 at 2024-12-06 ===\n",
      "VN30F1M = 13.624*CTG + 26.446*E1VFVN30 + 8.193*FUESSVFL\n",
      "\n",
      "=== New Combination 36 at 2024-12-10 ===\n",
      "VN30F1M = 7.314*CTG + 28.365*FUESSVFL + 10.570*E1VFVN30\n",
      "\n",
      "=== New Combination 37 at 2024-12-12 ===\n",
      "VN30F1M = 71.644*FUEVN100 + 3.480*STB\n",
      "\n",
      "=== New Combination 38 at 2024-12-17 ===\n",
      "VN30F1M = 75.297*FUEVN100\n",
      "\n",
      "=== New Combination 39 at 2024-12-18 ===\n",
      "VN30F1M = 75.779*FUEVN100\n",
      "\n",
      "=== New Combination 40 at 2024-12-26 ===\n",
      "VN30F1M = 77.725*FUEVN100\n",
      "\n",
      "=== New Combination 41 at 2024-12-30 ===\n",
      "VN30F1M = 78.451*FUEVN100\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "etf = 'VN30F1M'\n",
    "stocks = ['ACB', 'BCM', 'BID', 'BVH', 'CTG', 'FPT', 'GAS', 'GVR', 'HDB', 'HPG', 'LPB', 'MBB', 'MSN', 'MWG',\n",
    "            'PLX', 'SAB', 'SHB', 'SSI', 'STB', 'TCB', 'TPB', 'VCB', 'VHM', 'VIB', 'VIC', 'VJC', 'VNM', 'VRE',\n",
    "            'VPB', 'FUEVFVND', 'FUESSVFL', 'E1VFVN30', 'FUEVN100']\n",
    "start_date = '2024-01-01'\n",
    "end_date = '2024-12-31'\n",
    "file_path = 'data\\\\stock_data.csv'\n",
    "\n",
    "strategy = StatArbStrategy(etf, stocks, start_date, end_date, file_path)\n",
    "strategy.run_strategy()\n",
    "results_df = strategy.get_results()\n",
    "print(\"DONE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('draft\\\\results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignalGenerator:\n",
    "    def __init__(self, residuals: pd.DataFrame, ou_window: int = 60, fallback_days: int = 5):\n",
    "        self.residuals = residuals\n",
    "        self.ou_window = ou_window  # Increased default window\n",
    "        self.fallback_days = fallback_days  # Max days to carry forward parameters\n",
    "        self.ou_params = None\n",
    "        self.signals = None\n",
    "        self.last_valid_params = {col: None for col in residuals.columns}  # Track last valid params\n",
    "\n",
    "    def fit_ou_process(self, series: pd.Series, date: pd.Timestamp) -> Dict[str, float]:\n",
    "        \"\"\"Fit OU process with relaxed conditions.\"\"\"\n",
    "        if len(series) < self.ou_window:\n",
    "            return {'kappa': np.nan, 'm': np.nan, 'sigma': np.nan, 's_score': np.nan}\n",
    "        series_window = series[-self.ou_window:].dropna()\n",
    "        series_window = series_window.to_numpy()\n",
    "        if len(series_window) < self.ou_window:\n",
    "            return {'kappa': np.nan, 'm': np.nan, 'sigma': np.nan, 's_score': np.nan}\n",
    "        try:\n",
    "            model = AutoReg(series_window, lags=1).fit()\n",
    "            a, b = model.params\n",
    "            p_value_b = model.pvalues[1]\n",
    "            # Relaxed conditions: 0 < b < 1.05 and p-value < 0.10\n",
    "            if p_value_b >= 0.10 or b <= 0 or b >= 1:\n",
    "                return {'kappa': np.nan, 'm': np.nan, 'sigma': np.nan, 's_score': np.nan}\n",
    "            kappa = -np.log(b) * np.sqrt(252)\n",
    "            m = a / (1 - b)\n",
    "            sigma = np.sqrt(model.sigma2 * 2 * kappa / (1 - b**2))\n",
    "            latest = series.iloc[-1]\n",
    "            sigma_eq = sigma / np.sqrt(2 * kappa) if kappa > 0 else np.inf\n",
    "            s_score = (latest - m) / sigma_eq if sigma_eq != 0 else 0\n",
    "            return {'kappa': kappa, 'm': m, 'sigma': sigma, 's_score': s_score}\n",
    "        except (ValueError, np.linalg.LinAlgError):\n",
    "            return {'kappa': np.nan, 'm': np.nan, 'sigma': np.nan, 's_score': np.nan}\n",
    "\n",
    "    def apply_ou_fitting(self):\n",
    "        \"\"\"Fit OU process with fallback mechanism.\"\"\"\n",
    "        columns = pd.MultiIndex.from_product([self.residuals.columns, ['kappa', 'm', 'sigma', 's_score']])\n",
    "        self.ou_params = pd.DataFrame(index=self.residuals.index, columns=columns)\n",
    "        for t in range(self.ou_window, len(self.residuals)):\n",
    "            date = self.residuals.index[t]\n",
    "            for stock in self.residuals.columns:\n",
    "                series = self.residuals[stock].iloc[:t + 1]\n",
    "                params = self.fit_ou_process(series, date)\n",
    "                if not np.isnan(params['kappa']):\n",
    "                    # Store valid params with timestamp\n",
    "                    self.last_valid_params[stock] = {'params': params, 'date': date}\n",
    "                else:\n",
    "                    # Fallback to recent valid params\n",
    "                    if self.last_valid_params[stock] and (date - self.last_valid_params[stock]['date']).days <= self.fallback_days:\n",
    "                        last_params = self.last_valid_params[stock]['params']\n",
    "                        latest = series.iloc[-1]\n",
    "                        m = last_params['m']\n",
    "                        sigma_eq = last_params['sigma'] / np.sqrt(2 * last_params['kappa']) if last_params['kappa'] > 0 else np.inf\n",
    "                        params['s_score'] = (latest - m) / sigma_eq if sigma_eq != 0 else 0\n",
    "                for param, value in params.items():\n",
    "                    self.ou_params.loc[date, (stock, param)] = value\n",
    "    def count_valid_s_scores(self, verbose: bool = True) -> Dict[str, int]:\n",
    "        \"\"\"Count valid (non-NaN) s-scores in ou_params, overall and per series.\"\"\"\n",
    "        if self.ou_params is None:\n",
    "            raise ValueError(\"Run apply_ou_fitting() before counting s-scores.\")\n",
    "        # Total possible entries per series (excluding first ou_window rows)\n",
    "        total_possible = len(self.ou_params) - self.ou_window\n",
    "        # Count valid s-scores per series\n",
    "        s_score_counts = {}\n",
    "        total_score = 0  # Initialize total_score outside the loop\n",
    "        for stock in self.residuals.columns:\n",
    "            valid_s_scores = self.ou_params[(stock, 's_score')].dropna().count()\n",
    "            s_score_counts[stock] = valid_s_scores\n",
    "            total_score += valid_s_scores #Increment total_score\n",
    "            if verbose:\n",
    "                print(f\"Valid s-scores for {stock}: {valid_s_scores} out of {total_possible}\")\n",
    "        # Count total valid s-scores\n",
    "        total_valid_s_scores = self.ou_params.xs('s_score', level=1, axis=1).dropna().count().sum()\n",
    "        s_score_counts['Total'] = total_valid_s_scores\n",
    "        if verbose:\n",
    "            print(f\"Total valid s-scores: {total_valid_s_scores} out of {total_possible * len(self.residuals.columns)}\")\n",
    "        return s_score_counts, total_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot residuals: rows are Date, columns are Combination_ID, values are Residual\n",
    "residuals_pivot = results_df.pivot(index='Date', columns='Combination_ID', values='Residual')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit OU process\n",
    "signal_gen = SignalGenerator(residuals_pivot, ou_window=60)\n",
    "signal_gen.apply_ou_fitting()\n",
    "ou_params = signal_gen.ou_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "ou_params.to_csv('draft\\\\ou_params.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define thresholds\n",
    "upper_threshold = 1.25\n",
    "lower_threshold = -0.5\n",
    "# Continue until shorting until the lower_threshold change direction strongly (e.g. -1.5)\n",
    "# 1 --> small short, 1.25 --> bigger short kiểu z\n",
    "# Initialize trade_states DataFrame\n",
    "trade_states = pd.DataFrame(index=ou_params.index, columns=residuals_pivot.columns)\n",
    "\n",
    "# Generate trade states for each combination\n",
    "for comb_id in trade_states.columns:\n",
    "    s_scores = ou_params[(comb_id, 's_score')]\n",
    "    trade_state = 0  # 0: not in trade, 1: in trade\n",
    "    for date in s_scores.index:\n",
    "        s_score = s_scores[date]\n",
    "        if pd.isna(s_score) or pd.isna(residuals_pivot.loc[date, comb_id]):\n",
    "            trade_state = 0  # Combination not active\n",
    "        else:\n",
    "            if trade_state == 0 and s_score > upper_threshold:\n",
    "                trade_state = 1  # Enter trade\n",
    "            elif trade_state == 1 and s_score < lower_threshold:\n",
    "                trade_state = 0  # Exit trade\n",
    "        trade_states.loc[date, comb_id] = trade_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add s_score and Trade_State to results_df\n",
    "results_df['s_score'] = results_df.apply(\n",
    "    lambda row: ou_params.loc[row['Date'], (row['Combination_ID'], 's_score')] \n",
    "    if row['Date'] in ou_params.index else np.nan, axis=1\n",
    ")\n",
    "results_df['Trade_State'] = results_df.apply(\n",
    "    lambda row: trade_states.loc[row['Date'], row['Combination_ID']] \n",
    "    if row['Date'] in trade_states.index else 0, axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Combination_ID</th>\n",
       "      <th>Residual</th>\n",
       "      <th>Total_Combinations</th>\n",
       "      <th>Num_Stocks</th>\n",
       "      <th>Is_Estimation</th>\n",
       "      <th>Beta_E1VFVN30</th>\n",
       "      <th>Beta_MBB</th>\n",
       "      <th>Beta_FUESSVFL</th>\n",
       "      <th>Beta_GVR</th>\n",
       "      <th>...</th>\n",
       "      <th>Beta_VHM</th>\n",
       "      <th>Beta_TPB</th>\n",
       "      <th>Beta_TCB</th>\n",
       "      <th>Beta_FPT</th>\n",
       "      <th>Beta_BCM</th>\n",
       "      <th>Beta_SSI</th>\n",
       "      <th>Beta_GAS</th>\n",
       "      <th>Beta_STB</th>\n",
       "      <th>s_score</th>\n",
       "      <th>Trade_State</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-02</td>\n",
       "      <td>1</td>\n",
       "      <td>85.372056</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>50.941836</td>\n",
       "      <td>3.318821</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-01-03</td>\n",
       "      <td>1</td>\n",
       "      <td>91.191657</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>50.941836</td>\n",
       "      <td>3.318821</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-01-04</td>\n",
       "      <td>1</td>\n",
       "      <td>83.458534</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>50.941836</td>\n",
       "      <td>3.318821</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-01-05</td>\n",
       "      <td>1</td>\n",
       "      <td>87.754788</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>50.941836</td>\n",
       "      <td>3.318821</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-01-08</td>\n",
       "      <td>1</td>\n",
       "      <td>80.643497</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>50.941836</td>\n",
       "      <td>3.318821</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3094</th>\n",
       "      <td>2024-12-25</td>\n",
       "      <td>41</td>\n",
       "      <td>-54.174682</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3095</th>\n",
       "      <td>2024-12-26</td>\n",
       "      <td>41</td>\n",
       "      <td>-53.574682</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3096</th>\n",
       "      <td>2024-12-27</td>\n",
       "      <td>41</td>\n",
       "      <td>-62.050770</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3102</th>\n",
       "      <td>2024-12-30</td>\n",
       "      <td>41</td>\n",
       "      <td>-64.566259</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3108</th>\n",
       "      <td>2024-12-31</td>\n",
       "      <td>41</td>\n",
       "      <td>-60.343704</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3109 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date  Combination_ID   Residual  Total_Combinations  Num_Stocks  \\\n",
       "0    2024-01-02               1  85.372056                   1           2   \n",
       "1    2024-01-03               1  91.191657                   1           2   \n",
       "2    2024-01-04               1  83.458534                   1           2   \n",
       "3    2024-01-05               1  87.754788                   1           2   \n",
       "4    2024-01-08               1  80.643497                   1           2   \n",
       "...         ...             ...        ...                 ...         ...   \n",
       "3094 2024-12-25              41 -54.174682                   6           1   \n",
       "3095 2024-12-26              41 -53.574682                   6           1   \n",
       "3096 2024-12-27              41 -62.050770                   6           1   \n",
       "3102 2024-12-30              41 -64.566259                   6           1   \n",
       "3108 2024-12-31              41 -60.343704                   6           1   \n",
       "\n",
       "      Is_Estimation  Beta_E1VFVN30  Beta_MBB  Beta_FUESSVFL  Beta_GVR  ...  \\\n",
       "0              True      50.941836  3.318821            NaN       NaN  ...   \n",
       "1              True      50.941836  3.318821            NaN       NaN  ...   \n",
       "2              True      50.941836  3.318821            NaN       NaN  ...   \n",
       "3              True      50.941836  3.318821            NaN       NaN  ...   \n",
       "4              True      50.941836  3.318821            NaN       NaN  ...   \n",
       "...             ...            ...       ...            ...       ...  ...   \n",
       "3094           True            NaN       NaN            NaN       NaN  ...   \n",
       "3095           True            NaN       NaN            NaN       NaN  ...   \n",
       "3096           True            NaN       NaN            NaN       NaN  ...   \n",
       "3102          False            NaN       NaN            NaN       NaN  ...   \n",
       "3108          False            NaN       NaN            NaN       NaN  ...   \n",
       "\n",
       "      Beta_VHM  Beta_TPB  Beta_TCB  Beta_FPT  Beta_BCM  Beta_SSI  Beta_GAS  \\\n",
       "0          NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "1          NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "2          NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "3          NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "4          NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "3094       NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "3095       NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "3096       NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "3102       NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "3108       NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "\n",
       "      Beta_STB  s_score  Trade_State  \n",
       "0          NaN      NaN            0  \n",
       "1          NaN      NaN            0  \n",
       "2          NaN      NaN            0  \n",
       "3          NaN      NaN            0  \n",
       "4          NaN      NaN            0  \n",
       "...        ...      ...          ...  \n",
       "3094       NaN      NaN            0  \n",
       "3095       NaN      NaN            0  \n",
       "3096       NaN      NaN            0  \n",
       "3102       NaN      NaN            0  \n",
       "3108       NaN      NaN            0  \n",
       "\n",
       "[3109 rows x 29 columns]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n"
     ]
    }
   ],
   "source": [
    "# List of all stocks (you need to define this based on your portfolio)\n",
    "# Replace with your full stock list\n",
    "# Initialize positions_df\n",
    "dates = results_df['Date'].unique()\n",
    "positions_df = pd.DataFrame(index=dates)\n",
    "\n",
    "# Compute total positions\n",
    "positions_df['Position_VN30F1M'] = -results_df.groupby('Date')['Trade_State'].sum()\n",
    "for stock in stocks:\n",
    "    beta_col = f'Beta_{stock}'\n",
    "    if beta_col in results_df.columns:\n",
    "        positions_df[f'Position_{stock}'] = results_df.groupby('Date').apply(\n",
    "            lambda x: (x['Trade_State'] * x[beta_col]).sum()\n",
    "        )\n",
    "    else:\n",
    "        positions_df[f'Position_{stock}'] = 0  # If beta column missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('draft\\\\results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignalGenerator:\n",
    "    def __init__(self, residuals: pd.DataFrame, ou_window: int = 60, fallback_days: int = 5):\n",
    "        self.residuals = residuals\n",
    "        self.ou_window = ou_window\n",
    "        self.fallback_days = fallback_days\n",
    "        self.ou_params = None\n",
    "        self.last_valid_params = {col: None for col in residuals.columns}\n",
    "\n",
    "    def fit_ou_process(self, series: pd.Series, date: pd.Timestamp) -> Dict[str, float]:\n",
    "        if len(series) < self.ou_window:\n",
    "            return {'kappa': np.nan, 'm': np.nan, 'sigma': np.nan, 's_score': np.nan}\n",
    "        series_window = series[-self.ou_window:].dropna()\n",
    "        series_window = series_window.to_numpy()\n",
    "        if len(series_window) < self.ou_window:\n",
    "            return {'kappa': np.nan, 'm': np.nan, 'sigma': np.nan, 's_score': np.nan}\n",
    "        try:\n",
    "            model = AutoReg(series_window, lags=1).fit()\n",
    "            a, b = model.params\n",
    "            p_value_b = model.pvalues[1]\n",
    "            if p_value_b >= 0.10 or b <= 0 or b >= 1:\n",
    "                return {'kappa': np.nan, 'm': np.nan, 'sigma': np.nan, 's_score': np.nan}\n",
    "            kappa = -np.log(b) * np.sqrt(252)\n",
    "            m = a / (1 - b)\n",
    "            sigma = np.sqrt(model.sigma2 * 2 * kappa / (1 - b**2))\n",
    "            latest = series.iloc[-1]\n",
    "            sigma_eq = sigma / np.sqrt(2 * kappa) if kappa > 0 else np.inf\n",
    "            s_score = (latest - m) / sigma_eq if sigma_eq != 0 else 0\n",
    "            return {'kappa': kappa, 'm': m, 'sigma': sigma, 's_score': s_score}\n",
    "        except (ValueError, np.linalg.LinAlgError):\n",
    "            return {'kappa': np.nan, 'm': np.nan, 'sigma': np.nan, 's_score': np.nan}\n",
    "\n",
    "    def apply_ou_fitting(self):\n",
    "        columns = pd.MultiIndex.from_product([self.residuals.columns, ['kappa', 'm', 'sigma', 's_score']])\n",
    "        self.ou_params = pd.DataFrame(index=self.residuals.index, columns=columns)\n",
    "        for t in range(self.ou_window, len(self.residuals)):\n",
    "            date = self.residuals.index[t]\n",
    "            for stock in self.residuals.columns:\n",
    "                series = self.residuals[stock].iloc[:t + 1]\n",
    "                params = self.fit_ou_process(series, date)\n",
    "                if not np.isnan(params['kappa']):\n",
    "                    self.last_valid_params[stock] = {'params': params, 'date': date}\n",
    "                else:\n",
    "                    if self.last_valid_params[stock] and (date - self.last_valid_params[stock]['date']).days <= self.fallback_days:\n",
    "                        last_params = self.last_valid_params[stock]['params']\n",
    "                        latest = series.iloc[-1]\n",
    "                        m = last_params['m']\n",
    "                        sigma_eq = last_params['sigma'] / np.sqrt(2 * last_params['kappa']) if last_params['kappa'] > 0 else np.inf\n",
    "                        params['s_score'] = (latest - m) / sigma_eq if sigma_eq != 0 else 0\n",
    "                for param, value in params.items():\n",
    "                    self.ou_params.loc[date, (stock, param)] = value\n",
    "\n",
    "def get_allocation_tier(s_score: float, prev_allocation: float, prev_s_score: float = None) -> float:\n",
    "    \"\"\"\n",
    "    Determine allocation percentage based on s-score, considering previous allocation and s-score trend.\n",
    "    \"\"\"\n",
    "    # Cut-loss\n",
    "    if prev_s_score is not None and s_score < prev_s_score and prev_allocation > 0:\n",
    "            return prev_allocation  # Hold during continuous decline\n",
    "    # If s-score is between -0.5 and 0.5, hold position if decreasing, otherwise no position\n",
    "    elif s_score > 2.0:\n",
    "        return 0.5  # Reduce to 50%\n",
    "    # Entry tiers\n",
    "    elif s_score > 1.5:\n",
    "        return 1.0  # Full allocation\n",
    "    elif s_score > 1.0:\n",
    "        return 0.8\n",
    "    elif s_score > 0.5:\n",
    "        return 0.6\n",
    "    # Exit tiers\n",
    "    elif s_score < -0.5:\n",
    "        return 0.4  # Exit completely\n",
    "    elif s_score < -1.0:\n",
    "        return 0.2\n",
    "    elif s_score < -1.5:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "def process_results_df(results_df: pd.DataFrame, stocks: list, ou_window: int = 60) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    # Sort by Combination_ID\n",
    "    results_df = results_df.sort_values('Combination_ID')\n",
    "    \n",
    "    # Pivot residuals\n",
    "    residuals_pivot = results_df.pivot(index='Date', columns='Combination_ID', values='Residual')\n",
    "    \n",
    "    # Fit OU process\n",
    "    signal_gen = SignalGenerator(residuals_pivot, ou_window=ou_window)\n",
    "    signal_gen.apply_ou_fitting()\n",
    "    ou_params = signal_gen.ou_params\n",
    "    \n",
    "    # Generate allocation percentages\n",
    "    allocation_percentages = pd.DataFrame(index=ou_params.index, columns=residuals_pivot.columns)\n",
    "    for comb_id in allocation_percentages.columns:\n",
    "        s_scores = ou_params[(comb_id, 's_score')]\n",
    "        prev_allocation = 0.0\n",
    "        prev_s_score = None\n",
    "        for date in s_scores.index:\n",
    "            s_score = s_scores[date]\n",
    "            if pd.isna(s_score) or pd.isna(residuals_pivot.loc[date, comb_id]):\n",
    "                allocation = 0.0\n",
    "            else:\n",
    "                allocation = get_allocation_tier(s_score, prev_allocation, prev_s_score)\n",
    "            allocation_percentages.loc[date, comb_id] = allocation\n",
    "            prev_allocation = allocation\n",
    "            prev_s_score = s_score\n",
    "    \n",
    "    # Update results_df with s_score and Allocation\n",
    "    results_df['s_score'] = results_df.apply(\n",
    "        lambda row: ou_params.loc[row['Date'], (row['Combination_ID'], 's_score')] \n",
    "        if row['Date'] in ou_params.index else np.nan, axis=1\n",
    "    )\n",
    "    results_df['Allocation'] = results_df.apply(\n",
    "        lambda row: allocation_percentages.loc[row['Date'], row['Combination_ID']] \n",
    "        if row['Date'] in allocation_percentages.index else 0.0, axis=1\n",
    "    )\n",
    "    \n",
    "    # Compute positions_df with scaled allocations\n",
    "    dates = results_df['Date'].unique()\n",
    "    positions_df = pd.DataFrame(index=dates)\n",
    "    \n",
    "    for date in dates:\n",
    "        # Get active combinations with allocation > 0\n",
    "        active_combs = allocation_percentages.loc[date][allocation_percentages.loc[date] > 0]\n",
    "        scale_factor=1\n",
    "        if len(active_combs) == 0:\n",
    "            total_short = 0.0\n",
    "        else:\n",
    "            # Sum intended short positions\n",
    "            intended_shorts = active_combs.values\n",
    "            total_intended_short = sum(intended_shorts)\n",
    "            # Scale if exceeding 1.0\n",
    "            scale_factor = min(1.0 / total_intended_short, 1.0) if total_intended_short > 0 else 1.0\n",
    "            scaled_allocations = active_combs * scale_factor\n",
    "            total_short = sum(scaled_allocations)\n",
    "        \n",
    "        positions_df.loc[date, 'Position_VN30F1M'] = -total_short if total_short > 0 else 0.0\n",
    "        for stock in stocks:\n",
    "            beta_col = f'Beta_{stock}'\n",
    "            if beta_col in results_df.columns:\n",
    "                active_rows = results_df[(results_df['Date'] == date) & (results_df['Allocation'] > 0)]\n",
    "                positions_df.loc[date, f'Position_{stock}'] = (active_rows[beta_col] * active_rows['Allocation'] * scale_factor).sum()\n",
    "            else:\n",
    "                positions_df.loc[date, f'Position_{stock}'] = 0.0\n",
    "    \n",
    "    return results_df, positions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "updated_results_df, positions_df = process_results_df(results_df, stocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions_df.to_csv('data\\\\positions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_results_df.to_csv('result.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
