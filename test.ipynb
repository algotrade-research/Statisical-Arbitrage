{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get VN Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from vnstock import Vnstock\n",
    "from typing import List, Dict, Tuple\n",
    "import numpy as np\n",
    "from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import os\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.api import AutoReg\n",
    "from typing import Dict\n",
    "from scipy.stats import pearsonr\n",
    "from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import statsmodels.api as sm\n",
    "import os\n",
    "from vnstock import Vnstock  # Assuming this is the correct import for your data source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_data(symbols, start_date, end_date, interval='1D'):\n",
    "    stock_data = pd.DataFrame()  # Initialize empty DataFrame\n",
    "    for symbol in symbols:\n",
    "        # Fetch historical data for the symbol\n",
    "        stock = Vnstock().stock(symbol=symbol, source='VCI')\n",
    "        historical_data = stock.quote.history(\n",
    "            start=start_date, \n",
    "            end=end_date, \n",
    "            interval=interval\n",
    "        )\n",
    "        # Set 'time' as the index and keep only the 'Close' column\n",
    "        close_prices = historical_data[['close', 'time']].set_index('time')\n",
    "        close_prices = close_prices.rename(columns={'close': symbol})  # Rename column to symbol\n",
    "        # Concatenate with the main DataFrame\n",
    "        stock_data = pd.concat([stock_data, close_prices], axis=1).dropna()\n",
    "    return stock_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataHandler:\n",
    "    def __init__(self, futures, stocks, start_date, end_date, file_path, \n",
    "                 estimation_window=60, cluster_update_interval=5, \n",
    "                 futures_change_threshold=0.05, max_clusters=12):\n",
    "        \"\"\"Initialize the data handler for statistical arbitrage strategy.\n",
    "        \n",
    "        Parameters:\n",
    "        - futures: Futures symbol (e.g., VN30F1M)\n",
    "        - stocks: List of stock symbols\n",
    "        - start_date, end_date: Date range for data\n",
    "        - file_path: Path to store/load data\n",
    "        - estimation_window: Window size for rolling analysis\n",
    "        - cluster_update_interval: Days between clustering updates\n",
    "        - futures_change_threshold: Threshold for futures price change to trigger re-clustering\n",
    "        - max_clusters: Maximum number of clusters for K-means\n",
    "        \"\"\"\n",
    "        self.futures = futures\n",
    "        self.stocks = stocks\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date\n",
    "        self.file_path = file_path\n",
    "        self.estimation_window = estimation_window\n",
    "        self.cluster_update_interval = cluster_update_interval\n",
    "        self.futures_change_threshold = futures_change_threshold\n",
    "        self.max_clusters = max_clusters\n",
    "        self.data = self.load_data()\n",
    "        self.last_clusters = None\n",
    "        self.last_cluster_day = None\n",
    "        self.last_futures_price = None\n",
    "        \n",
    "    def load_data(self):\n",
    "        \"\"\"Load stock data from file or fetch if not available.\"\"\"\n",
    "        if os.path.exists(self.file_path):\n",
    "            data = pd.read_csv(self.file_path)\n",
    "        else:\n",
    "            data = self.get_stock_data([self.futures] + self.stocks, self.start_date, self.end_date, '1D')\n",
    "            data.to_csv(self.file_path, index=True)\n",
    "        data['Date'] = pd.to_datetime(data['time'])\n",
    "        return data.set_index('Date').dropna()\n",
    "\n",
    "    def compute_residuals(self, window_data):\n",
    "        \"\"\"Compute residuals from regressing each stock against the futures.\"\"\"\n",
    "        residuals = pd.DataFrame(index=window_data.index)\n",
    "        for stock in self.stocks:\n",
    "            if stock in window_data.columns:\n",
    "                X = sm.add_constant(window_data[self.futures])\n",
    "                y = window_data[stock]\n",
    "                model = sm.OLS(y, X).fit()\n",
    "                residuals[stock] = model.resid\n",
    "        return residuals.dropna()\n",
    "\n",
    "    def cluster_stocks(self, window_data, current_day, futures_current_price):\n",
    "        \"\"\"Cluster stocks, updating only every cluster_update_interval days or if futures changes significantly.\"\"\"\n",
    "        if self.last_clusters is not None and self.last_cluster_day is not None:\n",
    "            days_since_last_cluster = (current_day - self.last_cluster_day).days\n",
    "            futures_change = abs(futures_current_price - self.last_futures_price) / self.last_futures_price if self.last_futures_price else 0\n",
    "            if days_since_last_cluster < self.cluster_update_interval and futures_change < self.futures_change_threshold:\n",
    "                return self.last_clusters\n",
    "        residuals = self.compute_residuals(window_data)\n",
    "        if residuals.empty or len(residuals.columns) < 2:\n",
    "            self.last_clusters = [self.stocks]\n",
    "        else:\n",
    "            X = residuals.T\n",
    "            best_k = 2\n",
    "            best_score = -1\n",
    "            for k in range(2, min(self.max_clusters + 1, len(self.stocks))):\n",
    "                kmeans = KMeans(n_clusters=k, random_state=0).fit(X)\n",
    "                if kmeans.n_clusters > 1:\n",
    "                    score = silhouette_score(X, kmeans.labels_)\n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_k = k\n",
    "            kmeans = KMeans(n_clusters=best_k, random_state=0).fit(X)\n",
    "            clusters = {i: [] for i in range(best_k)}\n",
    "            for stock, label in zip(self.stocks, kmeans.labels_):\n",
    "                clusters[label].append(stock)\n",
    "            self.last_clusters = [cluster for cluster in clusters.values() if cluster]\n",
    "        self.last_cluster_day = current_day\n",
    "        self.last_futures_price = futures_current_price\n",
    "        return self.last_clusters\n",
    "\n",
    "class StatArbStrategy:    \n",
    "    def __init__(self, data_handler, min_trading_days=30, threshold=0.05,\n",
    "                 max_stocks=8, confidence_level=1, adf_significance=0.05,\n",
    "                 correlation_threshold=0.6, dynamic_threshold=True,\n",
    "                 residual_threshold=0.1):\n",
    "        \"\"\"Initialize the statistical arbitrage strategy.\n",
    "        \n",
    "        Parameters:\n",
    "        - data_handler: Instance of DataHandler for data management\n",
    "        - min_trading_days: Minimum days a combination must be traded\n",
    "        - threshold: Minimum beta threshold for inclusion\n",
    "        - max_stocks: Maximum stocks in a combination\n",
    "        - confidence_level: Confidence level for Johansen test (0=90%, 1=95%, 2=99%)\n",
    "        - adf_significance: Significance level for ADF test\n",
    "        - correlation_threshold: Threshold for residual similarity\n",
    "        - dynamic_threshold: Whether to adjust correlation threshold dynamically\n",
    "        - residual_threshold: Max allowed residual as fraction of futures value\n",
    "        \"\"\"\n",
    "        self.data_handler = data_handler\n",
    "        self.futures = data_handler.futures\n",
    "        self.stocks = data_handler.stocks\n",
    "        self.estimation_window = data_handler.estimation_window\n",
    "        self.data = data_handler.data\n",
    "        self.min_trading_days = min_trading_days\n",
    "        self.threshold = threshold\n",
    "        self.max_stocks = max_stocks\n",
    "        self.confidence_level = confidence_level\n",
    "        self.confidence_level_joh_final = min(2,confidence_level + 1)  # Derived\n",
    "        self.adf_significance = adf_significance\n",
    "        self.adf_significance_trading = min(0.1, 2 * adf_significance)  # Derived\n",
    "        self.correlation_threshold = correlation_threshold\n",
    "        self.dynamic_threshold = dynamic_threshold\n",
    "        self.residual_threshold = residual_threshold\n",
    "        self.active_combinations = []\n",
    "        self.combination_id = 0\n",
    "        self.results = []\n",
    "        self.validation_cache = {}\n",
    "\n",
    "    def get_pairwise_candidates(self, window_data, stocks_pool):\n",
    "        \"\"\"Perform pairwise Johansen tests on a pool of stocks.\"\"\"\n",
    "        candidates = []\n",
    "        for stock in stocks_pool:\n",
    "            try:\n",
    "                result = coint_johansen(window_data[[self.futures, stock]], det_order=1, k_ar_diff=1)\n",
    "                if result.lr1[0] > result.cvt[0, self.confidence_level]:\n",
    "                    candidates.append((stock, result.lr1[0]))\n",
    "            except Exception as e:\n",
    "                print(f\"Pairwise test failed for {stock}: {e}\")\n",
    "        candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "        return [stock for stock, _ in candidates]\n",
    "\n",
    "    def build_combination_greedy(self, window_data, candidates):\n",
    "        \"\"\"Greedily build a cointegrated combination from candidates with early stopping.\"\"\"\n",
    "        if not candidates:\n",
    "            return []\n",
    "        selected = [candidates[0]]\n",
    "        best_trace_stat = coint_johansen(window_data[[self.futures, selected[0]]], det_order=1, k_ar_diff=1).lr1[0]\n",
    "        for stock in candidates[1:]:\n",
    "            if len(selected) >= self.max_stocks:\n",
    "                break\n",
    "            test_subset = selected + [stock]\n",
    "            try:\n",
    "                result = coint_johansen(window_data[[self.futures] + test_subset], det_order=1, k_ar_diff=1)\n",
    "                if result.lr1[0] <= result.cvt[0, self.confidence_level]:\n",
    "                    continue\n",
    "                improvement = (result.lr1[0] - best_trace_stat) / best_trace_stat\n",
    "                if improvement < 0.05:  # Early stopping if improvement < 5%\n",
    "                    break\n",
    "                evec = result.evec[:, 0]\n",
    "                betas = -evec[1:] / evec[0]\n",
    "                if not all(beta >= 0 for beta in betas):\n",
    "                    continue\n",
    "                selected.append(stock)\n",
    "                best_trace_stat = result.lr1[0]\n",
    "            except Exception as e:\n",
    "                print(f\"Combination test failed: {e}\")\n",
    "        return selected\n",
    "\n",
    "    def validate_combination(self, window_data, selected):\n",
    "        \"\"\"Validate the combination with caching and residual constraints.\"\"\"\n",
    "        comb_key = frozenset(selected)\n",
    "        if comb_key in self.validation_cache:\n",
    "            return self.validation_cache[comb_key]\n",
    "        try:\n",
    "            # Johansen test with restricted intercept\n",
    "            result = coint_johansen(window_data[[self.futures] + list(selected)], det_order=1, k_ar_diff=1)\n",
    "            if result.lr1[0] <= result.cvt[0, self.confidence_level_joh_final]:\n",
    "                self.validation_cache[comb_key] = (None, np.inf)\n",
    "                return None, np.inf\n",
    "            evec = result.evec[:, 0]\n",
    "            betas = -evec[1:] / evec[0]\n",
    "            if not all(beta >= 0 for beta in betas):\n",
    "                self.validation_cache[comb_key] = (None, np.inf)\n",
    "                return None, np.inf\n",
    "            \n",
    "            # Compute residuals with Johansen betas\n",
    "            synthetic_portfolio = sum(window_data[s] * b for s, b in zip(selected, betas))\n",
    "            residuals = window_data[self.futures] - synthetic_portfolio\n",
    "            \n",
    "            # Estimate the intercept (mean of residuals, negated to match cointegration equation)\n",
    "            intercept = -residuals.mean()\n",
    "            \n",
    "            # Verify stationarity\n",
    "            adf_pvalue = adfuller(residuals)[1]\n",
    "            if adf_pvalue >= self.adf_significance:\n",
    "                self.validation_cache[comb_key] = (None, adf_pvalue)\n",
    "                return None, adf_pvalue\n",
    "            \n",
    "            # Check residual magnitude using 95th percentile\n",
    "            futures_avg = window_data[self.futures].mean()\n",
    "            if np.percentile(np.abs(residuals), 95) > self.residual_threshold * futures_avg:\n",
    "                self.validation_cache[comb_key] = (None, adf_pvalue)\n",
    "                return None, adf_pvalue\n",
    "            \n",
    "            # Filter betas by threshold\n",
    "            selected_betas = {s: b for s, b in zip(selected, betas) if abs(b) > self.threshold}\n",
    "            \n",
    "            # Package betas and intercept together\n",
    "            combination_params = {'intercept': intercept, 'betas': selected_betas}\n",
    "            self.validation_cache[comb_key] = (combination_params, adf_pvalue)\n",
    "            return combination_params, adf_pvalue\n",
    "        except Exception as e:\n",
    "            print(f\"Validation failed for {selected}: {e}\")\n",
    "            self.validation_cache[comb_key] = (None, np.inf)\n",
    "            return None, np.inf\n",
    "\n",
    "    def is_similar(self, new_residuals, existing_residuals):\n",
    "        \"\"\"Check if two residual series are highly correlated.\"\"\"\n",
    "        if len(new_residuals) != len(existing_residuals):\n",
    "            return False\n",
    "        corr, _ = pearsonr(new_residuals, existing_residuals)\n",
    "        return corr > self.correlation_threshold\n",
    "\n",
    "    def adjust_correlation_threshold(self):\n",
    "        \"\"\"Dynamically adjust correlation threshold based on active combinations.\"\"\"\n",
    "        if self.dynamic_threshold:\n",
    "            if len(self.active_combinations) < 10:\n",
    "                self.correlation_threshold = min(0.8, self.correlation_threshold + 0.05)\n",
    "            else:\n",
    "                self.correlation_threshold = max(0.5, self.correlation_threshold - 0.05)\n",
    "\n",
    "    def run_strategy(self):\n",
    "        \"\"\"Execute the rolling statistical arbitrage strategy.\"\"\"\n",
    "        for day in range(self.estimation_window, len(self.data)):\n",
    "            estimation_data = self.data.iloc[day - self.estimation_window:day]\n",
    "            current_day = self.data.index[day]\n",
    "            futures_current_price = self.data.iloc[day][self.futures]\n",
    "            self.adjust_correlation_threshold()\n",
    "            clusters = self.data_handler.cluster_stocks(estimation_data, current_day, futures_current_price)\n",
    "\n",
    "            # Within-cluster combinations\n",
    "            for cluster in clusters:\n",
    "                candidates = self.get_pairwise_candidates(estimation_data, cluster)\n",
    "                selected = self.build_combination_greedy(estimation_data, candidates)\n",
    "                if selected:\n",
    "                    params, new_adf_pvalue = self.validate_combination(estimation_data, selected)\n",
    "                    if params:\n",
    "                        self.add_combination_if_not_similar(params, new_adf_pvalue, estimation_data, current_day)\n",
    "\n",
    "            # Top candidates across clusters\n",
    "            top_candidates = []\n",
    "            for cluster in clusters:\n",
    "                cluster_candidates = self.get_pairwise_candidates(estimation_data, cluster)[:3]  # Top 3 per cluster\n",
    "                top_candidates.extend(cluster_candidates)\n",
    "            top_candidates = list(set(top_candidates))  # Remove duplicates\n",
    "\n",
    "            if top_candidates:\n",
    "                cross_selected = self.build_combination_greedy(estimation_data, top_candidates)\n",
    "                if cross_selected:\n",
    "                    cross_params, cross_adf_pvalue = self.validate_combination(estimation_data, cross_selected)\n",
    "                    if cross_params:\n",
    "                        self.add_combination_if_not_similar(cross_params, cross_adf_pvalue, estimation_data, current_day)\n",
    "\n",
    "            # Cross-cluster combinations\n",
    "            all_candidates = self.get_pairwise_candidates(estimation_data, self.stocks)\n",
    "            cross_selected = self.build_combination_greedy(estimation_data, all_candidates)\n",
    "            if cross_selected:\n",
    "                cross_params, cross_adf_pvalue = self.validate_combination(estimation_data, cross_selected)\n",
    "                if cross_params:\n",
    "                    self.add_combination_if_not_similar(cross_params, cross_adf_pvalue, estimation_data, current_day)\n",
    "\n",
    "            # Evaluate active combinations\n",
    "            for comb in self.active_combinations[:]:\n",
    "                if day < comb['start_day']:\n",
    "                    continue\n",
    "                comb['trading_days'] += 1\n",
    "                current_prices = self.data.iloc[day]\n",
    "                synthetic_portfolio = sum(current_prices[s] * b for s, b in comb['params']['betas'].items())\n",
    "                residual = current_prices[self.futures] - (comb['params']['intercept'] + synthetic_portfolio)\n",
    "                comb['all_residuals'].append(residual)\n",
    "                if comb['trading_days'] >= self.min_trading_days:\n",
    "                    recent_residuals = pd.Series(comb['all_residuals'][-self.estimation_window:])\n",
    "                    if adfuller(recent_residuals)[1] >= self.adf_significance_trading:\n",
    "                        self.active_combinations.remove(comb)\n",
    "                        continue\n",
    "                row = {\n",
    "                    'Date': current_day,\n",
    "                    'Combination_ID': comb['id'],\n",
    "                    'Residual': residual,\n",
    "                    'Total_Combinations': len(self.active_combinations),\n",
    "                    'Num_Stocks': len(comb['params']['betas']),\n",
    "                    'Is_Estimation': False,\n",
    "                    'Intercept': comb['params']['intercept'],\n",
    "                    **{f'Beta_{s}': b for s, b in comb['params']['betas'].items()}\n",
    "                }\n",
    "                self.results.append(row)\n",
    "\n",
    "    def add_combination_if_not_similar(self, params, new_adf_pvalue, estimation_data, current_day):\n",
    "        \"\"\"Add a new combination if it's not similar to existing ones.\"\"\"\n",
    "        synthetic_portfolio = sum(estimation_data[s] * b for s, b in params['betas'].items())\n",
    "        residuals = estimation_data[self.futures] - (params['intercept'] + synthetic_portfolio)\n",
    "        similar_found = False\n",
    "        to_remove = []\n",
    "        for comb in self.active_combinations:\n",
    "            existing_residuals = pd.Series(comb['all_residuals'][-self.estimation_window:])\n",
    "            if self.is_similar(residuals, existing_residuals):\n",
    "                if comb['trading_days'] >= self.min_trading_days:\n",
    "                    existing_adf_pvalue = adfuller(existing_residuals)[1]\n",
    "                    if new_adf_pvalue < 0.5 * existing_adf_pvalue:\n",
    "                        to_remove.append(comb)\n",
    "                else:\n",
    "                    similar_found = True\n",
    "        for comb in to_remove:\n",
    "            self.active_combinations.remove(comb)\n",
    "        if not similar_found:\n",
    "            self.combination_id += 1\n",
    "            self.active_combinations.append({\n",
    "                'id': self.combination_id,\n",
    "                'params': params,\n",
    "                'start_day': self.data.index.get_loc(current_day),\n",
    "                'all_residuals': residuals.tolist(),\n",
    "                'trading_days': 0\n",
    "            })\n",
    "            for i, res in enumerate(residuals):\n",
    "                row = {\n",
    "                    'Date': estimation_data.index[i],\n",
    "                    'Combination_ID': self.combination_id,\n",
    "                    'Residual': res,\n",
    "                    'Total_Combinations': len(self.active_combinations),\n",
    "                    'Num_Stocks': len(params['betas']),\n",
    "                    'Is_Estimation': True,\n",
    "                    'Intercept': params['intercept'],\n",
    "                    **{f'Beta_{s}': b for s, b in params['betas'].items()}\n",
    "                }\n",
    "                self.results.append(row)\n",
    "            print(f\"\\n=== New Combination {self.combination_id} at {current_day.date()} ===\")\n",
    "            print(f\"VN30F1M = {params['intercept']:.3f} + \" + \" + \".join([f\"{b:.3f}*{s}\" for s, b in params['betas'].items()]))\n",
    "\n",
    "    def get_results(self):\n",
    "        \"\"\"Return the results as a DataFrame.\"\"\"\n",
    "        self.results = pd.DataFrame(self.results)\n",
    "        self.results = self.results.sort_values(by=['Combination_ID', 'Date'])\n",
    "        return self.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== New Combination 1 at 2024-04-03 ===\n",
      "VN30F1M = 15.301 + 47.852*E1VFVN30 + 0.734*FUEVFVND + 0.930*ACB + 1.151*VJC + 1.465*VIB + 0.665*VCB\n",
      "\n",
      "=== New Combination 2 at 2024-04-03 ===\n",
      "VN30F1M = 105.392 + 41.458*E1VFVN30 + 1.776*FUEVFVND + 3.007*ACB + 1.700*VJC + 2.054*VIB + 1.531*VCB + 1.021*GVR\n",
      "\n",
      "=== New Combination 3 at 2024-04-11 ===\n",
      "VN30F1M = 25.967 + 8.528*E1VFVN30 + 40.259*FUEVN100 + 13.644*FUESSVFL + 1.669*MBB + 6.633*TPB\n",
      "\n",
      "=== New Combination 4 at 2024-04-12 ===\n",
      "VN30F1M = 50.015 + 25.961*E1VFVN30 + 25.804*FUEVN100 + 14.477*FUESSVFL + 0.298*STB\n",
      "\n",
      "=== New Combination 5 at 2024-04-17 ===\n",
      "VN30F1M = -92.187 + 5.952*STB + 45.117*E1VFVN30\n",
      "\n",
      "=== New Combination 6 at 2024-05-07 ===\n",
      "VN30F1M = 19.666 + 56.159*E1VFVN30 + 1.830*STB\n",
      "\n",
      "=== New Combination 7 at 2024-06-12 ===\n",
      "VN30F1M = 3.054 + 51.318*E1VFVN30 + 3.116*VIC\n",
      "\n",
      "=== New Combination 8 at 2024-06-18 ===\n",
      "VN30F1M = 44.750 + 59.481*E1VFVN30 + 0.150*VPB\n",
      "\n",
      "=== New Combination 9 at 2024-07-09 ===\n",
      "VN30F1M = 118.605 + 36.221*E1VFVN30 + 9.531*MBB + 32.120*SHB + 0.287*BCM + 4.566*TPB\n",
      "\n",
      "=== New Combination 10 at 2024-07-19 ===\n",
      "VN30F1M = -41.677 + 24.826*SSI + 11.836*VPB + 4.600*MWG + 2.024*FUEVFVND\n",
      "\n",
      "=== New Combination 11 at 2024-07-23 ===\n",
      "VN30F1M = -62.621 + 11.614*SSI + 19.651*VPB + 2.919*MWG + 1.950*VRE + 15.660*FUESSVFL\n",
      "\n",
      "=== New Combination 12 at 2024-08-08 ===\n",
      "VN30F1M = -3.124 + 57.757*E1VFVN30\n",
      "\n",
      "=== New Combination 13 at 2024-08-12 ===\n",
      "VN30F1M = -18.351 + 45.451*E1VFVN30 + 6.751*HPG + 0.771*STB + 1.356*ACB\n",
      "\n",
      "=== New Combination 14 at 2024-08-22 ===\n",
      "VN30F1M = 68.880 + 51.688*E1VFVN30 + 4.580*VHM + 0.415*TCB + 0.482*VRE\n",
      "\n",
      "=== New Combination 15 at 2024-08-30 ===\n",
      "VN30F1M = 60.134 + 41.792*VPB + 1.338*GAS + 10.882*TCB + 3.838*VCB\n",
      "\n",
      "=== New Combination 16 at 2024-09-12 ===\n",
      "VN30F1M = 94.296 + 30.620*VPB + 0.924*TCB + 0.708*BCM + 1.846*FPT + 21.067*ACB\n",
      "\n",
      "=== New Combination 17 at 2024-09-23 ===\n",
      "VN30F1M = -3.124 + 57.757*E1VFVN30\n",
      "\n",
      "=== New Combination 18 at 2024-10-04 ===\n",
      "VN30F1M = -12.722 + 5.105*FPT + 29.759*MBB\n",
      "\n",
      "=== New Combination 19 at 2024-10-08 ===\n",
      "VN30F1M = -30.561 + 13.028*GVR + 28.652*MBB + 3.195*BCM\n",
      "\n",
      "=== New Combination 20 at 2024-10-15 ===\n",
      "VN30F1M = 104.679 + 0.293*GVR + 3.019*MBB + 54.849*E1VFVN30 + 0.417*BCM + 2.372*TCB\n",
      "\n",
      "=== New Combination 21 at 2024-10-23 ===\n",
      "VN30F1M = -30.975 + 16.283*VCB + 7.114*TCB + 6.068*HDB\n",
      "\n",
      "=== New Combination 22 at 2024-11-04 ===\n",
      "VN30F1M = -8.976 + 55.979*E1VFVN30 + 0.725*SSI\n",
      "\n",
      "=== New Combination 23 at 2024-12-12 ===\n",
      "VN30F1M = -65.237 + 14.767*LPB + 30.236*MBB + 4.195*BID + 1.156*CTG\n",
      "\n",
      "=== New Combination 24 at 2024-12-20 ===\n",
      "VN30F1M = 11.537 + 52.648*E1VFVN30 + 6.060*FUESSVFL\n",
      "\n",
      "=== New Combination 25 at 2024-12-23 ===\n",
      "VN30F1M = -14.899 + 7.756*LPB + 14.787*TPB + 6.023*E1VFVN30 + 23.529*FUEVN100 + 13.773*MBB\n",
      "\n",
      "=== New Combination 26 at 2024-12-31 ===\n",
      "VN30F1M = 55.966 + 72.724*FUEVN100 + 4.913*VPB\n"
     ]
    }
   ],
   "source": [
    "futures = 'VN30F1M'\n",
    "stocks = ['ACB', 'BCM', 'BID', 'BVH', 'CTG', 'FPT', 'GAS', 'GVR', 'HDB', 'HPG', 'LPB', 'MBB', 'MSN', 'MWG',\n",
    "          'PLX', 'SAB', 'SHB', 'SSI', 'STB', 'TCB', 'TPB', 'VCB', 'VHM', 'VIB', 'VIC', 'VJC', 'VNM', 'VRE',\n",
    "          'VPB', 'FUEVFVND', 'FUESSVFL', 'E1VFVN30', 'FUEVN100']\n",
    "start_date = '2024-01-01'\n",
    "end_date = '2024-12-31'\n",
    "file_path = 'data\\\\stock_data.csv'\n",
    "data_handler = DataHandler(\n",
    "    futures=futures,\n",
    "    stocks=stocks, \n",
    "    start_date='2024-01-01',\n",
    "    end_date='2025-01-01',\n",
    "    file_path='stock_data.csv'\n",
    ")\n",
    "\n",
    "strategy = StatArbStrategy(data_handler)\n",
    "strategy.run_strategy()\n",
    "results_df = strategy.get_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('draft\\\\results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Trading Signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignalGenerator:\n",
    "    def __init__(self, residuals: pd.DataFrame, ou_window: int = 60, fallback_days: int = 5):\n",
    "        self.residuals = residuals\n",
    "        self.ou_window = ou_window\n",
    "        self.fallback_days = fallback_days\n",
    "        self.ou_params = None\n",
    "        self.last_valid_params = {col: None for col in residuals.columns}\n",
    "        self.ou_cache = {}  # Cache for OU parameters\n",
    "\n",
    "    def fit_ou_process(self, series: pd.Series, date: pd.Timestamp) -> Dict[str, float]:\n",
    "        \"\"\"Fit an Ornstein-Uhlenbeck process to the series and compute the s-score.\"\"\"\n",
    "        cache_key = (series.name, date)\n",
    "        if cache_key in self.ou_cache:\n",
    "            return self.ou_cache[cache_key]\n",
    "        if len(series) < self.ou_window:\n",
    "            return {'kappa': np.nan, 'm': np.nan, 'sigma': np.nan, 's_score': np.nan}\n",
    "        series_window = series[-self.ou_window:].dropna().to_numpy()\n",
    "        if len(series_window) < self.ou_window:\n",
    "            return {'kappa': np.nan, 'm': np.nan, 'sigma': np.nan, 's_score': np.nan}\n",
    "        try:\n",
    "            model = AutoReg(series_window, lags=1).fit()\n",
    "            a, b = model.params\n",
    "            p_value_b = model.pvalues[1]\n",
    "            if p_value_b >= 0.10 or b <= 0 or b >= 1:\n",
    "                return {'kappa': np.nan, 'm': np.nan, 'sigma': np.nan, 's_score': np.nan}\n",
    "            kappa = -np.log(b) * np.sqrt(252)  # Annualized mean reversion rate\n",
    "            m = a / (1 - b)  # Long-term mean\n",
    "            sigma = np.sqrt(model.sigma2 * 2 * kappa / (1 - b**2))  # Volatility\n",
    "            latest = series.iloc[-1]\n",
    "            sigma_eq = sigma / np.sqrt(2 * kappa) if kappa > 0 else np.inf\n",
    "            s_score = (latest - m) / sigma_eq if sigma_eq != 0 else 0\n",
    "            params = {'kappa': kappa, 'm': m, 'sigma': sigma, 's_score': s_score}\n",
    "            self.ou_cache[cache_key] = params\n",
    "            return params\n",
    "        except (ValueError, np.linalg.LinAlgError):\n",
    "            return {'kappa': np.nan, 'm': np.nan, 'sigma': np.nan, 's_score': np.nan}\n",
    "\n",
    "    def apply_ou_fitting(self):\n",
    "        \"\"\"Apply OU fitting across all residuals and dates.\"\"\"\n",
    "        columns = pd.MultiIndex.from_product([self.residuals.columns, ['kappa', 'm', 'sigma', 's_score']])\n",
    "        self.ou_params = pd.DataFrame(index=self.residuals.index, columns=columns)\n",
    "        for t in range(self.ou_window, len(self.residuals)):\n",
    "            date = self.residuals.index[t]\n",
    "            for stock in self.residuals.columns:\n",
    "                series = self.residuals[stock].iloc[:t + 1]\n",
    "                params = self.fit_ou_process(series, date)\n",
    "                if not np.isnan(params['kappa']):\n",
    "                    self.last_valid_params[stock] = {'params': params, 'date': date}\n",
    "                elif self.last_valid_params[stock] and (date - self.last_valid_params[stock]['date']).days <= self.fallback_days:\n",
    "                    last_params = self.last_valid_params[stock]['params']\n",
    "                    latest = series.iloc[-1]\n",
    "                    m, kappa, sigma = last_params['m'], last_params['kappa'], last_params['sigma']\n",
    "                    sigma_eq = sigma / np.sqrt(2 * kappa) if kappa > 0 else np.inf\n",
    "                    params['s_score'] = (latest - m) / sigma_eq if sigma_eq != 0 else 0\n",
    "                for param, value in params.items():\n",
    "                    self.ou_params.loc[date, (stock, param)] = value\n",
    "\n",
    "def get_allocation_tier(s_score: float, prev_allocation: float, prev_s_score: float, is_decreasing_trend: bool) -> float:\n",
    "    \"\"\"\n",
    "    Determine allocation percentage based on s-score levels and trends.\n",
    "\n",
    "    Args:\n",
    "        s_score (float): Current s-score\n",
    "        prev_allocation (float): Previous allocation percentage\n",
    "        prev_s_score (float): Previous s-score\n",
    "        is_decreasing_trend (bool): Whether the s-score is in a decreasing trend\n",
    "\n",
    "    Returns:\n",
    "        float: Allocation percentage between 0.0 and 1.25\n",
    "    \"\"\"\n",
    "    if s_score > 2.0:\n",
    "        return 0.0  # Cut loss\n",
    "    elif s_score > 1.5 and s_score > prev_s_score:\n",
    "        return 1.25  # Increase position if rising\n",
    "    elif s_score > 1.25 and s_score > prev_s_score:\n",
    "        return 1.0  # Increase position if rising\n",
    "    elif s_score > 1.0:\n",
    "        return 0.75  # Start position when s-score is high\n",
    "    elif s_score < -1.5:\n",
    "        return 0.0  # Full exit\n",
    "    elif s_score < prev_s_score and prev_allocation > 0 and not is_decreasing_trend:\n",
    "        # Take profit if decreasing but not in a continuous trend\n",
    "        if 0.5 <= s_score <= 1.2:\n",
    "            return max(0.4, prev_allocation - 0.2)  # Gradual reduction\n",
    "        return prev_allocation\n",
    "    return prev_allocation  # Hold if in decreasing trend or no significant change\n",
    "\n",
    "def process_results_df(results_df: pd.DataFrame, stocks: list, ou_window: int = 60, lockup_days: int = 2) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Process results to generate s-scores, positions, and trading log with a lockup period.\n",
    "\n",
    "    Args:\n",
    "        results_df (pd.DataFrame): DataFrame with Date, Combination_ID, Residual, and Beta columns\n",
    "        stocks (list): List of stock symbols\n",
    "        ou_window (int): Window size for OU process fitting\n",
    "        lockup_days (int): Minimum holding period for combinations\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing:\n",
    "            - results_df (pd.DataFrame): Updated with s_score and Allocation\n",
    "            - positions_df (pd.DataFrame): Positions for VN30F1M and each stock\n",
    "            - trading_log (pd.DataFrame): Detailed trading actions with amounts bought/sold\n",
    "    \"\"\"\n",
    "    results_df = results_df.sort_values('Combination_ID')\n",
    "    residuals_pivot = results_df.pivot(index='Date', columns='Combination_ID', values='Residual')\n",
    "\n",
    "    # Fit OU process\n",
    "    signal_gen = SignalGenerator(residuals_pivot, ou_window=ou_window)\n",
    "    signal_gen.apply_ou_fitting()\n",
    "    ou_params = signal_gen.ou_params\n",
    "\n",
    "    # Generate allocation percentages with lockup and trend logic\n",
    "    allocation_percentages = pd.DataFrame(index=ou_params.index, columns=residuals_pivot.columns, dtype=float).fillna(0.0)\n",
    "    lockup_tracker = {comb_id: None for comb_id in residuals_pivot.columns}  # Entry date for lockup\n",
    "    trend_tracker = {comb_id: False for comb_id in residuals_pivot.columns}  # Decreasing trend flag\n",
    "\n",
    "    for comb_id in allocation_percentages.columns:\n",
    "        s_scores = ou_params[(comb_id, 's_score')]\n",
    "        prev_allocation = 0.0\n",
    "        prev_s_score = np.nan\n",
    "        for i, date in enumerate(s_scores.index):\n",
    "            if i < ou_window:\n",
    "                allocation = 0.0\n",
    "            else:\n",
    "                s_score = s_scores[date]\n",
    "                if pd.isna(s_score) or pd.isna(residuals_pivot.loc[date, comb_id]):\n",
    "                    allocation = 0.0\n",
    "                else:\n",
    "                    is_decreasing = s_score < prev_s_score if not pd.isna(prev_s_score) else False\n",
    "                    trend_tracker[comb_id] = is_decreasing and trend_tracker[comb_id] if not pd.isna(prev_s_score) else False\n",
    "                    intended_allocation = get_allocation_tier(s_score, prev_allocation, prev_s_score, trend_tracker[comb_id])\n",
    "                    \n",
    "                    # Enforce lockup period\n",
    "                    if intended_allocation > 0:\n",
    "                        if prev_allocation == 0:\n",
    "                            lockup_tracker[comb_id] = date  # Start lockup\n",
    "                        allocation = intended_allocation  # Take the intended position\n",
    "                    elif intended_allocation == 0 and prev_allocation > 0:\n",
    "                        if lockup_days > 0 and lockup_tracker[comb_id] and (date - lockup_tracker[comb_id]).days < lockup_days:\n",
    "                            allocation = prev_allocation  # Hold due to lockup\n",
    "                        else:\n",
    "                            allocation = 0.0\n",
    "                            lockup_tracker[comb_id] = None  # Reset lockup\n",
    "                    else:\n",
    "                        allocation = 0.0  # No position if intended_allocation == 0 and prev_allocation == 0\n",
    "                    \n",
    "                    # Update prev_s_score for next iteration\n",
    "                    prev_s_score = s_score if not pd.isna(s_score) else prev_s_score\n",
    "            allocation_percentages.loc[date, comb_id] = allocation\n",
    "            prev_allocation = allocation\n",
    "\n",
    "    # Update results_df\n",
    "    results_df['s_score'] = results_df.apply(\n",
    "        lambda row: ou_params.loc[row['Date'], (row['Combination_ID'], 's_score')]\n",
    "        if row['Date'] in ou_params.index else np.nan, axis=1\n",
    "    )\n",
    "    results_df['Allocation'] = results_df.apply(\n",
    "        lambda row: allocation_percentages.loc[row['Date'], row['Combination_ID']]\n",
    "        if row['Date'] in allocation_percentages.index else 0.0, axis=1\n",
    "    )\n",
    "\n",
    "    # Compute positions_df with scaled allocations\n",
    "    dates = results_df['Date'].unique()\n",
    "    positions_df = pd.DataFrame(index=dates, dtype=float).fillna(0.0)\n",
    "    trading_log = pd.DataFrame(index=dates, dtype=float)\n",
    "\n",
    "    for date in dates:\n",
    "        active_combs = allocation_percentages.loc[date][allocation_percentages.loc[date] > 0]\n",
    "        num_active = len(active_combs)\n",
    "        # Initialize scale_factor to a default value\n",
    "        scale_factor = 1.0\n",
    "        if num_active == 0:\n",
    "            total_short = 0.0\n",
    "        else:\n",
    "            # Each combination gets an equal portfolio weight, scaled by intended allocation\n",
    "            scale_factor = 1.0 / num_active if num_active > 0 else 1.0\n",
    "            scaled_allocations = active_combs * scale_factor\n",
    "            total_short = scaled_allocations.sum()\n",
    "        \n",
    "        positions_df.loc[date, 'Position_VN30F1M'] = -total_short if total_short > 0 else 0.0\n",
    "        for stock in stocks:\n",
    "            beta_col = f'Beta_{stock}'\n",
    "            if beta_col in results_df.columns:\n",
    "                active_rows = results_df[(results_df['Date'] == date) & (results_df['Allocation'] > 0)]\n",
    "                positions_df.loc[date, f'Position_{stock}'] = (active_rows[beta_col] * active_rows['Allocation'] * scale_factor).sum()\n",
    "            else:\n",
    "                positions_df.loc[date, f'Position_{stock}'] = 0.0\n",
    "\n",
    "    # Generate trading_log with portfolio allocation changes\n",
    "    assets = ['VN30F1M'] + stocks\n",
    "    for asset in assets:\n",
    "        pos_col = f'Position_{asset}'\n",
    "        trading_log[f'Delta_{asset}'] = positions_df[pos_col].diff().fillna(0.0)\n",
    "        trading_log[f'Action_{asset}'] = np.where(trading_log[f'Delta_{asset}'] > 0, 'buy',\n",
    "                                                  np.where(trading_log[f'Delta_{asset}'] < 0, 'sell', 'hold'))\n",
    "\n",
    "    return results_df, positions_df, trading_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df, positions_df, trading_log = process_results_df(results_df, stocks=stocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions_df.to_csv('signal\\\\positions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('signal\\\\result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "trading_log.to_csv('signal\\\\trading_log.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Porfolio Management\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
